{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== begin of gnn configuration ======\n",
      "| msg_average = 0\n",
      "======   end of gnn configuration ======\n",
      "Namespace(attr_dim=0, batch_size=1, conv1d_activation='ReLU', data='deepfake', dropout=True, edge_feat_dim=0, extract_features=True, feat_dim=0, fff='1', fold=1, gm='DGCNN', hidden=128, latent_dim=[32, 32, 32, 1], learning_rate=0.0001, max_lv=4, mode='gpu', num_class=0, num_epochs=100, out_dim=0, printAUC=True, save_dir='/home/jupyter/saved_models', seed=1, sortpooling_k=0.6, test_number=2600)\n",
      "loading data\n",
      "# classes: 2\n",
      "# maximum node tag: 9\n",
      "# train: 10549, # test: 2600\n",
      "k used in SortPooling is: 72\n",
      "Initializing DGCNN\n",
      "loss: 0.00001 acc: 1.00000: 100%|██████| 10549/10549 [01:52<00:00, 93.96batch/s]\n",
      "\u001b[92maverage training of epoch 0: loss 0.26189 acc 0.90786 auc 0.92276\u001b[0m\n",
      "loss: 2.01234 acc: 0.00000: 100%|███████| 2600/2600 [00:12<00:00, 206.24batch/s]\n",
      "\u001b[93maverage test of epoch 0: loss 0.24680 acc 0.90962 auc 0.91279\u001b[0m\n",
      "----saving to best model since this is the best valid loss so far.----\n",
      "loss: 0.24477 acc: 1.00000: 100%|██████| 10549/10549 [01:51<00:00, 94.31batch/s]\n",
      "\u001b[92maverage training of epoch 1: loss 0.24885 acc 0.91184 auc 0.92515\u001b[0m\n",
      "loss: 1.56944 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 197.06batch/s]\n",
      "\u001b[93maverage test of epoch 1: loss 0.24387 acc 0.90923 auc 0.93073\u001b[0m\n",
      "----saving to best model since this is the best valid loss so far.----\n",
      "loss: 0.23155 acc: 1.00000: 100%|██████| 10549/10549 [02:05<00:00, 84.07batch/s]\n",
      "\u001b[92maverage training of epoch 2: loss 0.24814 acc 0.91165 auc 0.92312\u001b[0m\n",
      "loss: 1.51918 acc: 0.00000: 100%|███████| 2600/2600 [00:14<00:00, 183.93batch/s]\n",
      "\u001b[93maverage test of epoch 2: loss 0.24221 acc 0.90962 auc 0.93214\u001b[0m\n",
      "----saving to best model since this is the best valid loss so far.----\n",
      "loss: 0.00000 acc: 1.00000: 100%|██████| 10549/10549 [02:06<00:00, 83.60batch/s]\n",
      "\u001b[92maverage training of epoch 3: loss 0.24752 acc 0.91165 auc 0.92427\u001b[0m\n",
      "loss: 1.76467 acc: 0.00000: 100%|███████| 2600/2600 [00:14<00:00, 182.72batch/s]\n",
      "\u001b[93maverage test of epoch 3: loss 0.24397 acc 0.91000 auc 0.92890\u001b[0m\n",
      "loss: 0.00000 acc: 1.00000: 100%|██████| 10549/10549 [02:09<00:00, 81.52batch/s]\n",
      "\u001b[92maverage training of epoch 4: loss 0.24769 acc 0.91156 auc 0.92211\u001b[0m\n",
      "loss: 1.41734 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 188.49batch/s]\n",
      "\u001b[93maverage test of epoch 4: loss 0.24850 acc 0.91000 auc 0.93069\u001b[0m\n",
      "loss: 0.00009 acc: 1.00000: 100%|██████| 10549/10549 [02:07<00:00, 82.53batch/s]\n",
      "\u001b[92maverage training of epoch 5: loss 0.25080 acc 0.91137 auc 0.92148\u001b[0m\n",
      "loss: 2.24530 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 187.39batch/s]\n",
      "\u001b[93maverage test of epoch 5: loss 0.24969 acc 0.91000 auc 0.91510\u001b[0m\n",
      "loss: 0.00309 acc: 1.00000: 100%|██████| 10549/10549 [02:08<00:00, 82.35batch/s]\n",
      "\u001b[92maverage training of epoch 6: loss 0.25011 acc 0.91184 auc 0.92230\u001b[0m\n",
      "loss: 1.91669 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 189.24batch/s]\n",
      "\u001b[93maverage test of epoch 6: loss 0.24553 acc 0.90962 auc 0.92892\u001b[0m\n",
      "loss: 0.18275 acc: 1.00000: 100%|██████| 10549/10549 [02:07<00:00, 82.77batch/s]\n",
      "\u001b[92maverage training of epoch 7: loss 0.24594 acc 0.91165 auc 0.92394\u001b[0m\n",
      "loss: 1.64678 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 194.68batch/s]\n",
      "\u001b[93maverage test of epoch 7: loss 0.24265 acc 0.90923 auc 0.92866\u001b[0m\n",
      "loss: 0.23796 acc: 1.00000: 100%|██████| 10549/10549 [01:59<00:00, 88.11batch/s]\n",
      "\u001b[92maverage training of epoch 8: loss 0.24793 acc 0.91099 auc 0.92117\u001b[0m\n",
      "loss: 1.53616 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 192.17batch/s]\n",
      "\u001b[93maverage test of epoch 8: loss 0.24283 acc 0.91000 auc 0.92826\u001b[0m\n",
      "loss: 0.21304 acc: 1.00000: 100%|██████| 10549/10549 [02:01<00:00, 86.68batch/s]\n",
      "\u001b[92maverage training of epoch 9: loss 0.24559 acc 0.91156 auc 0.92523\u001b[0m\n",
      "loss: 1.66101 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 189.83batch/s]\n",
      "\u001b[93maverage test of epoch 9: loss 0.24300 acc 0.90962 auc 0.93191\u001b[0m\n",
      "loss: 0.12533 acc: 1.00000: 100%|██████| 10549/10549 [02:00<00:00, 87.39batch/s]\n",
      "\u001b[92maverage training of epoch 10: loss 0.24588 acc 0.91156 auc 0.92173\u001b[0m\n",
      "loss: 1.63230 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 190.42batch/s]\n",
      "\u001b[93maverage test of epoch 10: loss 0.24348 acc 0.91000 auc 0.93236\u001b[0m\n",
      "loss: 0.00000 acc: 1.00000: 100%|██████| 10549/10549 [02:01<00:00, 86.86batch/s]\n",
      "\u001b[92maverage training of epoch 11: loss 0.24336 acc 0.91212 auc 0.92503\u001b[0m\n",
      "loss: 1.83841 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 193.71batch/s]\n",
      "\u001b[93maverage test of epoch 11: loss 0.24583 acc 0.91000 auc 0.93131\u001b[0m\n",
      "loss: 0.00000 acc: 1.00000: 100%|██████| 10549/10549 [02:01<00:00, 87.05batch/s]\n",
      "\u001b[92maverage training of epoch 12: loss 0.24552 acc 0.91165 auc 0.92363\u001b[0m\n",
      "loss: 1.64540 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 192.81batch/s]\n",
      "\u001b[93maverage test of epoch 12: loss 0.24273 acc 0.91000 auc 0.93188\u001b[0m\n",
      "loss: 0.00000 acc: 1.00000: 100%|██████| 10549/10549 [02:01<00:00, 86.88batch/s]\n",
      "\u001b[92maverage training of epoch 13: loss 0.24567 acc 0.91184 auc 0.92344\u001b[0m\n",
      "loss: 1.70501 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 186.53batch/s]\n",
      "\u001b[93maverage test of epoch 13: loss 0.24349 acc 0.90923 auc 0.93264\u001b[0m\n",
      "loss: 0.00000 acc: 1.00000: 100%|██████| 10549/10549 [02:01<00:00, 86.86batch/s]\n",
      "\u001b[92maverage training of epoch 14: loss 0.24900 acc 0.91156 auc 0.92097\u001b[0m\n",
      "loss: 1.74440 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 197.09batch/s]\n",
      "\u001b[93maverage test of epoch 14: loss 0.24362 acc 0.90962 auc 0.93165\u001b[0m\n",
      "loss: 0.13688 acc: 1.00000: 100%|██████| 10549/10549 [02:04<00:00, 84.58batch/s]\n",
      "\u001b[92maverage training of epoch 15: loss 0.25030 acc 0.91193 auc 0.92501\u001b[0m\n",
      "loss: 1.73250 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 187.06batch/s]\n",
      "\u001b[93maverage test of epoch 15: loss 0.24379 acc 0.90962 auc 0.93194\u001b[0m\n",
      "loss: 0.00000 acc: 1.00000: 100%|██████| 10549/10549 [02:03<00:00, 85.27batch/s]\n",
      "\u001b[92maverage training of epoch 16: loss 0.24737 acc 0.91193 auc 0.92464\u001b[0m\n",
      "loss: 1.57004 acc: 0.00000: 100%|███████| 2600/2600 [00:12<00:00, 201.26batch/s]\n",
      "\u001b[93maverage test of epoch 16: loss 0.24346 acc 0.91000 auc 0.93330\u001b[0m\n",
      "loss: 0.00000 acc: 1.00000: 100%|██████| 10549/10549 [02:01<00:00, 86.56batch/s]\n",
      "\u001b[92maverage training of epoch 17: loss 0.24425 acc 0.91175 auc 0.92319\u001b[0m\n",
      "loss: 1.52277 acc: 0.00000: 100%|███████| 2600/2600 [00:13<00:00, 195.08batch/s]\n",
      "\u001b[93maverage test of epoch 17: loss 0.24732 acc 0.90923 auc 0.92757\u001b[0m\n",
      "loss: 0.27789 acc: 1.00000: 100%|██████| 10549/10549 [03:07<00:00, 56.37batch/s]\n",
      "\u001b[92maverage training of epoch 18: loss 0.24690 acc 0.91156 auc 0.92432\u001b[0m\n",
      "loss: 1.53030 acc: 0.00000: 100%|████████| 2600/2600 [00:30<00:00, 86.61batch/s]\n",
      "\u001b[93maverage test of epoch 18: loss 0.24162 acc 0.91000 auc 0.93050\u001b[0m\n",
      "----saving to best model since this is the best valid loss so far.----\n",
      "loss: 0.00000 acc: 1.00000: 100%|██████| 10549/10549 [03:43<00:00, 47.18batch/s]\n",
      "\u001b[92maverage training of epoch 19: loss 0.24561 acc 0.91184 auc 0.92191\u001b[0m\n",
      "loss: 1.61229 acc: 0.00000: 100%|████████| 2600/2600 [00:29<00:00, 86.97batch/s]\n",
      "\u001b[93maverage test of epoch 19: loss 0.24250 acc 0.90962 auc 0.93366\u001b[0m\n",
      "loss: 1.64275 acc: 0.00000: 100%|██████| 10549/10549 [03:22<00:00, 52.01batch/s]\n",
      "\u001b[92maverage training of epoch 20: loss 0.24815 acc 0.91156 auc 0.92431\u001b[0m\n",
      "loss: 1.53996 acc: 0.00000: 100%|███████| 2600/2600 [00:20<00:00, 123.97batch/s]\n",
      "\u001b[93maverage test of epoch 20: loss 0.24539 acc 0.90962 auc 0.92812\u001b[0m\n",
      "loss: 0.14230 acc: 1.00000: 100%|██████| 10549/10549 [02:58<00:00, 59.05batch/s]\n",
      "\u001b[92maverage training of epoch 21: loss 0.24303 acc 0.91203 auc 0.92471\u001b[0m\n",
      "loss: 1.80426 acc: 0.00000: 100%|████████| 2600/2600 [00:29<00:00, 87.11batch/s]\n",
      "\u001b[93maverage test of epoch 21: loss 0.25513 acc 0.90962 auc 0.92278\u001b[0m\n",
      "loss: 0.00000 acc: 1.00000: 100%|██████| 10549/10549 [03:41<00:00, 47.63batch/s]\n",
      "\u001b[92maverage training of epoch 22: loss 0.24452 acc 0.91127 auc 0.92485\u001b[0m\n",
      "loss: 1.49024 acc: 0.00000: 100%|████████| 2600/2600 [00:29<00:00, 86.90batch/s]\n",
      "\u001b[93maverage test of epoch 22: loss 0.24906 acc 0.90962 auc 0.92944\u001b[0m\n",
      "loss: 0.23982 acc: 1.00000: 100%|██████| 10549/10549 [03:41<00:00, 47.55batch/s]\n",
      "\u001b[92maverage training of epoch 23: loss 0.24425 acc 0.91165 auc 0.92352\u001b[0m\n",
      "loss: 1.51777 acc: 0.00000: 100%|████████| 2600/2600 [00:29<00:00, 86.82batch/s]\n",
      "\u001b[93maverage test of epoch 23: loss 0.24538 acc 0.90962 auc 0.93009\u001b[0m\n",
      "loss: 0.00000 acc: 1.00000:  84%|█████▉ | 8899/10549 [03:08<00:34, 47.94batch/s]"
     ]
    }
   ],
   "source": [
    "# Try with AUC now\n",
    "# Train on the Full FF dataset with 2K test graphs\n",
    "!./run_DGCNN.sh deepfake 1 2600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
