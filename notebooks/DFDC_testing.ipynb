{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/DGCNN\n"
     ]
    }
   ],
   "source": [
    "%cd DGCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change permissions, this should be resolved now\n",
    "!chmod +x run_DGCNN.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== begin of gnn configuration ======\n",
      "| msg_average = 0\n",
      "======   end of gnn configuration ======\n",
      "Namespace(batch_size=1, conv1d_activation='ReLU', data='MUTAG', dropout=True, edge_feat_dim=0, extract_features=False, feat_dim=0, fold=1, gm='DGCNN', hidden=128, latent_dim=[32, 32, 32, 1], learning_rate=0.0001, max_lv=4, mode='gpu', num_class=0, num_epochs=30, out_dim=0, printAUC=False, save_dir='DGCNN/saved', seed=1, sortpooling_k=0.6, test_number=20)\n",
      "loading data\n",
      "# classes: 2\n",
      "# maximum node tag: 7\n",
      "# train: 168, # test: 20\n",
      "k used in SortPooling is: 19\n",
      "Initializing DGCNN\n",
      "loss: 0.28383 acc: 1.00000: 100%|██████████| 168/168 [00:02<00:00, 68.40batch/s]\n",
      "\u001b[92maverage training of epoch 0: loss 0.57997 acc 0.74405 auc 0.00000\u001b[0m\n",
      "loss: 1.00750 acc: 0.00000: 100%|███████████| 20/20 [00:00<00:00, 242.73batch/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:800: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:88: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(dx < 0):\n",
      "\u001b[93maverage test of epoch 0: loss 1.19069 acc 0.00000 auc 0.00000\u001b[0m\n",
      "loss: 0.34252 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 108.21batch/s]\n",
      "\u001b[92maverage training of epoch 1: loss 0.51085 acc 0.74405 auc 0.00000\u001b[0m\n",
      "loss: 0.91164 acc: 0.00000: 100%|███████████| 20/20 [00:00<00:00, 249.36batch/s]\n",
      "\u001b[93maverage test of epoch 1: loss 1.21063 acc 0.00000 auc 0.00000\u001b[0m\n",
      "loss: 0.22982 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 109.92batch/s]\n",
      "\u001b[92maverage training of epoch 2: loss 0.46707 acc 0.74405 auc 0.00000\u001b[0m\n",
      "loss: 0.77887 acc: 0.00000: 100%|███████████| 20/20 [00:00<00:00, 247.65batch/s]\n",
      "\u001b[93maverage test of epoch 2: loss 1.17769 acc 0.00000 auc 0.00000\u001b[0m\n",
      "loss: 0.02663 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 110.95batch/s]\n",
      "\u001b[92maverage training of epoch 3: loss 0.43913 acc 0.75000 auc 0.00000\u001b[0m\n",
      "loss: 0.74651 acc: 0.00000: 100%|███████████| 20/20 [00:00<00:00, 257.30batch/s]\n",
      "\u001b[93maverage test of epoch 3: loss 1.32250 acc 0.00000 auc 0.00000\u001b[0m\n",
      "loss: 0.29850 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 106.40batch/s]\n",
      "\u001b[92maverage training of epoch 4: loss 0.43405 acc 0.79762 auc 0.00000\u001b[0m\n",
      "loss: 0.57472 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 236.20batch/s]\n",
      "\u001b[93maverage test of epoch 4: loss 1.09913 acc 0.35000 auc 0.00000\u001b[0m\n",
      "loss: 0.05355 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 107.70batch/s]\n",
      "\u001b[92maverage training of epoch 5: loss 0.39273 acc 0.84524 auc 0.00000\u001b[0m\n",
      "loss: 0.48451 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 249.23batch/s]\n",
      "\u001b[93maverage test of epoch 5: loss 1.04949 acc 0.40000 auc 0.00000\u001b[0m\n",
      "loss: 0.06640 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 106.19batch/s]\n",
      "\u001b[92maverage training of epoch 6: loss 0.39350 acc 0.84524 auc 0.00000\u001b[0m\n",
      "loss: 0.40708 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 241.97batch/s]\n",
      "\u001b[93maverage test of epoch 6: loss 1.00215 acc 0.50000 auc 0.00000\u001b[0m\n",
      "loss: 0.05652 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 108.62batch/s]\n",
      "\u001b[92maverage training of epoch 7: loss 0.39031 acc 0.85714 auc 0.00000\u001b[0m\n",
      "loss: 0.40262 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 239.79batch/s]\n",
      "\u001b[93maverage test of epoch 7: loss 0.99022 acc 0.50000 auc 0.00000\u001b[0m\n",
      "loss: 1.00496 acc: 0.00000: 100%|█████████| 168/168 [00:01<00:00, 105.49batch/s]\n",
      "\u001b[92maverage training of epoch 8: loss 0.37975 acc 0.86905 auc 0.00000\u001b[0m\n",
      "loss: 0.34777 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 238.32batch/s]\n",
      "\u001b[93maverage test of epoch 8: loss 0.90516 acc 0.55000 auc 0.00000\u001b[0m\n",
      "loss: 0.01213 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 108.42batch/s]\n",
      "\u001b[92maverage training of epoch 9: loss 0.39866 acc 0.85119 auc 0.00000\u001b[0m\n",
      "loss: 0.39132 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 248.36batch/s]\n",
      "\u001b[93maverage test of epoch 9: loss 1.06103 acc 0.50000 auc 0.00000\u001b[0m\n",
      "loss: 0.27969 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 106.55batch/s]\n",
      "\u001b[92maverage training of epoch 10: loss 0.37355 acc 0.86905 auc 0.00000\u001b[0m\n",
      "loss: 0.34603 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 240.81batch/s]\n",
      "\u001b[93maverage test of epoch 10: loss 0.91764 acc 0.55000 auc 0.00000\u001b[0m\n",
      "loss: 0.08215 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 105.54batch/s]\n",
      "\u001b[92maverage training of epoch 11: loss 0.36948 acc 0.85119 auc 0.00000\u001b[0m\n",
      "loss: 0.35388 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 241.63batch/s]\n",
      "\u001b[93maverage test of epoch 11: loss 0.94501 acc 0.55000 auc 0.00000\u001b[0m\n",
      "loss: 0.08348 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 106.99batch/s]\n",
      "\u001b[92maverage training of epoch 12: loss 0.36335 acc 0.86905 auc 0.00000\u001b[0m\n",
      "loss: 0.32398 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 234.84batch/s]\n",
      "\u001b[93maverage test of epoch 12: loss 0.84420 acc 0.60000 auc 0.00000\u001b[0m\n",
      "loss: 0.09173 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 106.30batch/s]\n",
      "\u001b[92maverage training of epoch 13: loss 0.34135 acc 0.86905 auc 0.00000\u001b[0m\n",
      "loss: 0.29085 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 244.89batch/s]\n",
      "\u001b[93maverage test of epoch 13: loss 0.79320 acc 0.60000 auc 0.00000\u001b[0m\n",
      "loss: 0.04070 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 106.13batch/s]\n",
      "\u001b[92maverage training of epoch 14: loss 0.35685 acc 0.85714 auc 0.00000\u001b[0m\n",
      "loss: 0.27835 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 236.26batch/s]\n",
      "\u001b[93maverage test of epoch 14: loss 0.64628 acc 0.70000 auc 0.00000\u001b[0m\n",
      "loss: 0.15998 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 108.12batch/s]\n",
      "\u001b[92maverage training of epoch 15: loss 0.36757 acc 0.86905 auc 0.00000\u001b[0m\n",
      "loss: 0.33958 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 244.01batch/s]\n",
      "\u001b[93maverage test of epoch 15: loss 1.05079 acc 0.55000 auc 0.00000\u001b[0m\n",
      "loss: 0.02614 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 107.99batch/s]\n",
      "\u001b[92maverage training of epoch 16: loss 0.35124 acc 0.88095 auc 0.00000\u001b[0m\n",
      "loss: 0.29932 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 242.87batch/s]\n",
      "\u001b[93maverage test of epoch 16: loss 0.84958 acc 0.60000 auc 0.00000\u001b[0m\n",
      "loss: 1.66902 acc: 0.00000: 100%|█████████| 168/168 [00:01<00:00, 108.38batch/s]\n",
      "\u001b[92maverage training of epoch 17: loss 0.34572 acc 0.86905 auc 0.00000\u001b[0m\n",
      "loss: 0.27498 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 248.97batch/s]\n",
      "\u001b[93maverage test of epoch 17: loss 0.75780 acc 0.65000 auc 0.00000\u001b[0m\n",
      "loss: 0.04408 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 111.50batch/s]\n",
      "\u001b[92maverage training of epoch 18: loss 0.35235 acc 0.88095 auc 0.00000\u001b[0m\n",
      "loss: 0.29365 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 248.84batch/s]\n",
      "\u001b[93maverage test of epoch 18: loss 0.88442 acc 0.60000 auc 0.00000\u001b[0m\n",
      "loss: 0.02376 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 105.06batch/s]\n",
      "\u001b[92maverage training of epoch 19: loss 0.33574 acc 0.88095 auc 0.00000\u001b[0m\n",
      "loss: 0.31732 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 241.84batch/s]\n",
      "\u001b[93maverage test of epoch 19: loss 1.00251 acc 0.55000 auc 0.00000\u001b[0m\n",
      "loss: 0.04598 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 106.07batch/s]\n",
      "\u001b[92maverage training of epoch 20: loss 0.34490 acc 0.88095 auc 0.00000\u001b[0m\n",
      "loss: 0.28648 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 250.17batch/s]\n",
      "\u001b[93maverage test of epoch 20: loss 0.74311 acc 0.65000 auc 0.00000\u001b[0m\n",
      "loss: 0.11951 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 107.80batch/s]\n",
      "\u001b[92maverage training of epoch 21: loss 0.33579 acc 0.88690 auc 0.00000\u001b[0m\n",
      "loss: 0.31985 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 235.90batch/s]\n",
      "\u001b[93maverage test of epoch 21: loss 1.03276 acc 0.55000 auc 0.00000\u001b[0m\n",
      "loss: 0.11508 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 106.59batch/s]\n",
      "\u001b[92maverage training of epoch 22: loss 0.33644 acc 0.88690 auc 0.00000\u001b[0m\n",
      "loss: 0.28974 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 236.33batch/s]\n",
      "\u001b[93maverage test of epoch 22: loss 0.87092 acc 0.60000 auc 0.00000\u001b[0m\n",
      "loss: 0.05933 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 109.05batch/s]\n",
      "\u001b[92maverage training of epoch 23: loss 0.31951 acc 0.88690 auc 0.00000\u001b[0m\n",
      "loss: 0.28343 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 254.11batch/s]\n",
      "\u001b[93maverage test of epoch 23: loss 0.94977 acc 0.60000 auc 0.00000\u001b[0m\n",
      "loss: 0.23263 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 106.75batch/s]\n",
      "\u001b[92maverage training of epoch 24: loss 0.32758 acc 0.88690 auc 0.00000\u001b[0m\n",
      "loss: 0.29606 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 245.37batch/s]\n",
      "\u001b[93maverage test of epoch 24: loss 0.92779 acc 0.60000 auc 0.00000\u001b[0m\n",
      "loss: 0.03092 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 108.26batch/s]\n",
      "\u001b[92maverage training of epoch 25: loss 0.32227 acc 0.87500 auc 0.00000\u001b[0m\n",
      "loss: 0.26697 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 248.82batch/s]\n",
      "\u001b[93maverage test of epoch 25: loss 0.80928 acc 0.65000 auc 0.00000\u001b[0m\n",
      "loss: 0.10253 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 107.05batch/s]\n",
      "\u001b[92maverage training of epoch 26: loss 0.32371 acc 0.88095 auc 0.00000\u001b[0m\n",
      "loss: 0.30197 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 244.17batch/s]\n",
      "\u001b[93maverage test of epoch 26: loss 1.00175 acc 0.60000 auc 0.00000\u001b[0m\n",
      "loss: 0.23192 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 107.85batch/s]\n",
      "\u001b[92maverage training of epoch 27: loss 0.32127 acc 0.88690 auc 0.00000\u001b[0m\n",
      "loss: 0.25206 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 252.28batch/s]\n",
      "\u001b[93maverage test of epoch 27: loss 0.69420 acc 0.65000 auc 0.00000\u001b[0m\n",
      "loss: 0.02026 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 105.26batch/s]\n",
      "\u001b[92maverage training of epoch 28: loss 0.32421 acc 0.89881 auc 0.00000\u001b[0m\n",
      "loss: 0.27795 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 241.52batch/s]\n",
      "\u001b[93maverage test of epoch 28: loss 0.87639 acc 0.65000 auc 0.00000\u001b[0m\n",
      "loss: 0.07230 acc: 1.00000: 100%|█████████| 168/168 [00:01<00:00, 107.61batch/s]\n",
      "\u001b[92maverage training of epoch 29: loss 0.32440 acc 0.88690 auc 0.00000\u001b[0m\n",
      "loss: 0.28669 acc: 1.00000: 100%|███████████| 20/20 [00:00<00:00, 250.12batch/s]\n",
      "\u001b[93maverage test of epoch 29: loss 0.91215 acc 0.65000 auc 0.00000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Quick test on a protein dataset to ensure the structure is correct\n",
    "!./run_DGCNN.sh MUTAG 1 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !./run_DGCNN.sh DFDC_sample 1 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== begin of gnn configuration ======\n",
      "| msg_average = 0\n",
      "======   end of gnn configuration ======\n",
      "Namespace(batch_size=1, conv1d_activation='ReLU', data='DFDC', dropout=True, edge_feat_dim=0, extract_features=False, feat_dim=0, fold=1, gm='DGCNN', hidden=128, latent_dim=[32, 32, 32, 1], learning_rate=0.0001, max_lv=4, mode='gpu', num_class=0, num_epochs=30, out_dim=0, printAUC=False, save_dir='DGCNN/saved', seed=1, sortpooling_k=0.6, test_number=700)\n",
      "loading data\n",
      "# classes: 2\n",
      "# maximum node tag: 9\n",
      "# train: 2767, # test: 700\n",
      "k used in SortPooling is: 72\n",
      "Initializing DGCNN\n",
      "loss: 0.47519 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 95.88batch/s]\n",
      "\u001b[92maverage training of epoch 0: loss 0.54420 acc 0.76075 auc 0.00000\u001b[0m\n",
      "loss: 1.77944 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 215.11batch/s]\n",
      "\u001b[93maverage test of epoch 0: loss 0.53307 acc 0.75000 auc 0.00000\u001b[0m\n",
      "loss: 0.48816 acc: 1.00000: 100%|████████| 2767/2767 [00:27<00:00, 98.82batch/s]\n",
      "\u001b[92maverage training of epoch 1: loss 0.52346 acc 0.77701 auc 0.00000\u001b[0m\n",
      "loss: 1.60643 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 216.48batch/s]\n",
      "\u001b[93maverage test of epoch 1: loss 0.53890 acc 0.74429 auc 0.00000\u001b[0m\n",
      "loss: 0.62409 acc: 1.00000: 100%|████████| 2767/2767 [00:27<00:00, 99.19batch/s]\n",
      "\u001b[92maverage training of epoch 2: loss 0.51688 acc 0.77846 auc 0.00000\u001b[0m\n",
      "loss: 1.69180 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 217.80batch/s]\n",
      "\u001b[93maverage test of epoch 2: loss 0.53856 acc 0.74857 auc 0.00000\u001b[0m\n",
      "loss: 0.11772 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 100.44batch/s]\n",
      "\u001b[92maverage training of epoch 3: loss 0.52334 acc 0.77774 auc 0.00000\u001b[0m\n",
      "loss: 1.81332 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 218.31batch/s]\n",
      "\u001b[93maverage test of epoch 3: loss 0.55472 acc 0.74000 auc 0.00000\u001b[0m\n",
      "loss: 0.16555 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 100.12batch/s]\n",
      "\u001b[92maverage training of epoch 4: loss 0.51377 acc 0.78171 auc 0.00000\u001b[0m\n",
      "loss: 1.92516 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 218.23batch/s]\n",
      "\u001b[93maverage test of epoch 4: loss 0.58098 acc 0.74143 auc 0.00000\u001b[0m\n",
      "loss: 2.02296 acc: 0.00000: 100%|████████| 2767/2767 [00:27<00:00, 99.13batch/s]\n",
      "\u001b[92maverage training of epoch 5: loss 0.50701 acc 0.78244 auc 0.00000\u001b[0m\n",
      "loss: 1.36229 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 217.33batch/s]\n",
      "\u001b[93maverage test of epoch 5: loss 0.53290 acc 0.76143 auc 0.00000\u001b[0m\n",
      "loss: 0.11668 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 95.78batch/s]\n",
      "\u001b[92maverage training of epoch 6: loss 0.48880 acc 0.79617 auc 0.00000\u001b[0m\n",
      "loss: 1.52167 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 210.72batch/s]\n",
      "\u001b[93maverage test of epoch 6: loss 0.53786 acc 0.73571 auc 0.00000\u001b[0m\n",
      "loss: 0.02130 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 96.06batch/s]\n",
      "\u001b[92maverage training of epoch 7: loss 0.45437 acc 0.80087 auc 0.00000\u001b[0m\n",
      "loss: 1.54584 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 210.47batch/s]\n",
      "\u001b[93maverage test of epoch 7: loss 0.43614 acc 0.80143 auc 0.00000\u001b[0m\n",
      "loss: 0.32355 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 97.10batch/s]\n",
      "\u001b[92maverage training of epoch 8: loss 0.42565 acc 0.81424 auc 0.00000\u001b[0m\n",
      "loss: 1.35762 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 205.50batch/s]\n",
      "\u001b[93maverage test of epoch 8: loss 0.40805 acc 0.82571 auc 0.00000\u001b[0m\n",
      "loss: 0.38505 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 97.29batch/s]\n",
      "\u001b[92maverage training of epoch 9: loss 0.40254 acc 0.82400 auc 0.00000\u001b[0m\n",
      "loss: 2.09489 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 212.50batch/s]\n",
      "\u001b[93maverage test of epoch 9: loss 0.42789 acc 0.79143 auc 0.00000\u001b[0m\n",
      "loss: 0.00001 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 98.28batch/s]\n",
      "\u001b[92maverage training of epoch 10: loss 0.38947 acc 0.83809 auc 0.00000\u001b[0m\n",
      "loss: 1.38440 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 210.97batch/s]\n",
      "\u001b[93maverage test of epoch 10: loss 0.38466 acc 0.84571 auc 0.00000\u001b[0m\n",
      "loss: 1.92625 acc: 0.00000: 100%|████████| 2767/2767 [00:28<00:00, 97.74batch/s]\n",
      "\u001b[92maverage training of epoch 11: loss 0.38306 acc 0.83773 auc 0.00000\u001b[0m\n",
      "loss: 2.55847 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 209.30batch/s]\n",
      "\u001b[93maverage test of epoch 11: loss 0.48265 acc 0.81286 auc 0.00000\u001b[0m\n",
      "loss: 0.02568 acc: 1.00000: 100%|████████| 2767/2767 [00:27<00:00, 99.16batch/s]\n",
      "\u001b[92maverage training of epoch 12: loss 0.37660 acc 0.84062 auc 0.00000\u001b[0m\n",
      "loss: 2.07165 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 215.64batch/s]\n",
      "\u001b[93maverage test of epoch 12: loss 0.41383 acc 0.82714 auc 0.00000\u001b[0m\n",
      "loss: 0.13083 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 100.89batch/s]\n",
      "\u001b[92maverage training of epoch 13: loss 0.38054 acc 0.84424 auc 0.00000\u001b[0m\n",
      "loss: 1.89636 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 217.92batch/s]\n",
      "\u001b[93maverage test of epoch 13: loss 0.37292 acc 0.85857 auc 0.00000\u001b[0m\n",
      "loss: 0.35171 acc: 1.00000: 100%|████████| 2767/2767 [00:27<00:00, 99.46batch/s]\n",
      "\u001b[92maverage training of epoch 14: loss 0.38628 acc 0.84207 auc 0.00000\u001b[0m\n",
      "loss: 1.59937 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 213.73batch/s]\n",
      "\u001b[93maverage test of epoch 14: loss 0.37204 acc 0.85000 auc 0.00000\u001b[0m\n",
      "loss: 0.13042 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 98.38batch/s]\n",
      "\u001b[92maverage training of epoch 15: loss 0.38357 acc 0.84821 auc 0.00000\u001b[0m\n",
      "loss: 1.71530 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 211.38batch/s]\n",
      "\u001b[93maverage test of epoch 15: loss 0.36688 acc 0.85143 auc 0.00000\u001b[0m\n",
      "loss: 0.19878 acc: 1.00000: 100%|████████| 2767/2767 [00:27<00:00, 99.57batch/s]\n",
      "\u001b[92maverage training of epoch 16: loss 0.37098 acc 0.85291 auc 0.00000\u001b[0m\n",
      "loss: 1.59485 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 216.79batch/s]\n",
      "\u001b[93maverage test of epoch 16: loss 0.44167 acc 0.81286 auc 0.00000\u001b[0m\n",
      "loss: 0.07709 acc: 1.00000: 100%|████████| 2767/2767 [00:27<00:00, 99.90batch/s]\n",
      "\u001b[92maverage training of epoch 17: loss 0.37323 acc 0.84243 auc 0.00000\u001b[0m\n",
      "loss: 2.25004 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 219.07batch/s]\n",
      "\u001b[93maverage test of epoch 17: loss 0.40711 acc 0.84000 auc 0.00000\u001b[0m\n",
      "loss: 0.20857 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 102.15batch/s]\n",
      "\u001b[92maverage training of epoch 18: loss 0.37960 acc 0.84532 auc 0.00000\u001b[0m\n",
      "loss: 1.31238 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 222.55batch/s]\n",
      "\u001b[93maverage test of epoch 18: loss 0.40415 acc 0.83571 auc 0.00000\u001b[0m\n",
      "loss: 0.22921 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 103.37batch/s]\n",
      "\u001b[92maverage training of epoch 19: loss 0.37496 acc 0.84496 auc 0.00000\u001b[0m\n",
      "loss: 2.42256 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 221.59batch/s]\n",
      "\u001b[93maverage test of epoch 19: loss 0.40039 acc 0.85286 auc 0.00000\u001b[0m\n",
      "loss: 0.29955 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 102.89batch/s]\n",
      "\u001b[92maverage training of epoch 20: loss 0.38777 acc 0.83845 auc 0.00000\u001b[0m\n",
      "loss: 1.80052 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 226.17batch/s]\n",
      "\u001b[93maverage test of epoch 20: loss 0.42260 acc 0.82571 auc 0.00000\u001b[0m\n",
      "loss: 0.11034 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 103.53batch/s]\n",
      "\u001b[92maverage training of epoch 21: loss 0.37565 acc 0.84785 auc 0.00000\u001b[0m\n",
      "loss: 2.32221 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 225.08batch/s]\n",
      "\u001b[93maverage test of epoch 21: loss 0.44632 acc 0.82000 auc 0.00000\u001b[0m\n",
      "loss: 0.12463 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 103.93batch/s]\n",
      "\u001b[92maverage training of epoch 22: loss 0.37150 acc 0.84857 auc 0.00000\u001b[0m\n",
      "loss: 2.12321 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 225.11batch/s]\n",
      "\u001b[93maverage test of epoch 22: loss 0.39621 acc 0.83429 auc 0.00000\u001b[0m\n",
      "loss: 0.54620 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 103.68batch/s]\n",
      "\u001b[92maverage training of epoch 23: loss 0.37372 acc 0.84640 auc 0.00000\u001b[0m\n",
      "loss: 1.65563 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 227.34batch/s]\n",
      "\u001b[93maverage test of epoch 23: loss 0.39271 acc 0.84286 auc 0.00000\u001b[0m\n",
      "loss: 0.22971 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 104.17batch/s]\n",
      "\u001b[92maverage training of epoch 24: loss 0.38703 acc 0.84062 auc 0.00000\u001b[0m\n",
      "loss: 1.89089 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 223.34batch/s]\n",
      "\u001b[93maverage test of epoch 24: loss 0.44214 acc 0.80286 auc 0.00000\u001b[0m\n",
      "loss: 0.49745 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 101.97batch/s]\n",
      "\u001b[92maverage training of epoch 25: loss 0.36894 acc 0.85399 auc 0.00000\u001b[0m\n",
      "loss: 2.22245 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 224.65batch/s]\n",
      "\u001b[93maverage test of epoch 25: loss 0.37974 acc 0.85429 auc 0.00000\u001b[0m\n",
      "loss: 0.13418 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 103.30batch/s]\n",
      "\u001b[92maverage training of epoch 26: loss 0.37395 acc 0.84821 auc 0.00000\u001b[0m\n",
      "loss: 2.16442 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 225.87batch/s]\n",
      "\u001b[93maverage test of epoch 26: loss 0.40017 acc 0.83571 auc 0.00000\u001b[0m\n",
      "loss: 0.16147 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 103.74batch/s]\n",
      "\u001b[92maverage training of epoch 27: loss 0.36988 acc 0.84966 auc 0.00000\u001b[0m\n",
      "loss: 2.42436 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 222.72batch/s]\n",
      "\u001b[93maverage test of epoch 27: loss 0.49309 acc 0.80143 auc 0.00000\u001b[0m\n",
      "loss: 0.14271 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 103.59batch/s]\n",
      "\u001b[92maverage training of epoch 28: loss 0.38230 acc 0.84062 auc 0.00000\u001b[0m\n",
      "loss: 1.99340 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 221.65batch/s]\n",
      "\u001b[93maverage test of epoch 28: loss 0.42887 acc 0.82857 auc 0.00000\u001b[0m\n",
      "loss: 0.71143 acc: 0.00000: 100%|███████| 2767/2767 [00:26<00:00, 103.63batch/s]\n",
      "\u001b[92maverage training of epoch 29: loss 0.36539 acc 0.84966 auc 0.00000\u001b[0m\n",
      "loss: 2.12268 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 223.47batch/s]\n",
      "\u001b[93maverage test of epoch 29: loss 0.42490 acc 0.83143 auc 0.00000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!./run_DGCNN.sh DFDC 1 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== begin of gnn configuration ======\n",
      "| msg_average = 0\n",
      "======   end of gnn configuration ======\n",
      "Namespace(batch_size=1, conv1d_activation='ReLU', data='DFDC', dropout=True, edge_feat_dim=0, extract_features=False, feat_dim=0, fold=1, gm='DGCNN', hidden=128, latent_dim=[32, 32, 32, 1], learning_rate=0.0001, max_lv=4, mode='gpu', num_class=0, num_epochs=30, out_dim=0, printAUC=True, save_dir='DGCNN/saved', seed=1, sortpooling_k=0.6, test_number=700)\n",
      "loading data\n",
      "# classes: 2\n",
      "# maximum node tag: 9\n",
      "# train: 2767, # test: 700\n",
      "k used in SortPooling is: 72\n",
      "Initializing DGCNN\n",
      "loss: 0.47519 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 100.79batch/s]\n",
      "\u001b[92maverage training of epoch 0: loss 0.54420 acc 0.76075 auc 0.73728\u001b[0m\n",
      "loss: 1.77944 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 223.03batch/s]\n",
      "\u001b[93maverage test of epoch 0: loss 0.53307 acc 0.75000 auc 0.77690\u001b[0m\n",
      "loss: 0.48816 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 98.54batch/s]\n",
      "\u001b[92maverage training of epoch 1: loss 0.52346 acc 0.77701 auc 0.76035\u001b[0m\n",
      "loss: 1.60643 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 215.27batch/s]\n",
      "\u001b[93maverage test of epoch 1: loss 0.53890 acc 0.74429 auc 0.78604\u001b[0m\n",
      "loss: 0.62410 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 97.18batch/s]\n",
      "\u001b[92maverage training of epoch 2: loss 0.51688 acc 0.77846 auc 0.76218\u001b[0m\n",
      "loss: 1.69181 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 207.15batch/s]\n",
      "\u001b[93maverage test of epoch 2: loss 0.53856 acc 0.74857 auc 0.76947\u001b[0m\n",
      "loss: 0.11772 acc: 1.00000: 100%|████████| 2767/2767 [00:27<00:00, 99.03batch/s]\n",
      "\u001b[92maverage training of epoch 3: loss 0.52334 acc 0.77774 auc 0.75590\u001b[0m\n",
      "loss: 1.81336 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 217.31batch/s]\n",
      "\u001b[93maverage test of epoch 3: loss 0.55472 acc 0.74000 auc 0.80494\u001b[0m\n",
      "loss: 0.16054 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 96.52batch/s]\n",
      "\u001b[92maverage training of epoch 4: loss 0.51347 acc 0.78207 auc 0.76921\u001b[0m\n",
      "loss: 1.88878 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 216.34batch/s]\n",
      "\u001b[93maverage test of epoch 4: loss 0.57631 acc 0.74429 auc 0.77228\u001b[0m\n",
      "loss: 0.68603 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 96.16batch/s]\n",
      "\u001b[92maverage training of epoch 5: loss 0.50241 acc 0.78605 auc 0.78078\u001b[0m\n",
      "loss: 1.44603 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 206.92batch/s]\n",
      "\u001b[93maverage test of epoch 5: loss 0.51693 acc 0.79429 auc 0.79995\u001b[0m\n",
      "loss: 0.09250 acc: 1.00000: 100%|████████| 2767/2767 [00:29<00:00, 94.86batch/s]\n",
      "\u001b[92maverage training of epoch 6: loss 0.48593 acc 0.79328 auc 0.79154\u001b[0m\n",
      "loss: 1.45990 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 212.87batch/s]\n",
      "\u001b[93maverage test of epoch 6: loss 0.47796 acc 0.77857 auc 0.82682\u001b[0m\n",
      "loss: 0.05366 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 95.43batch/s]\n",
      "\u001b[92maverage training of epoch 7: loss 0.45307 acc 0.80448 auc 0.81077\u001b[0m\n",
      "loss: 1.50083 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 207.67batch/s]\n",
      "\u001b[93maverage test of epoch 7: loss 0.45996 acc 0.80000 auc 0.82885\u001b[0m\n",
      "loss: 0.29403 acc: 1.00000: 100%|████████| 2767/2767 [00:28<00:00, 96.92batch/s]\n",
      "\u001b[92maverage training of epoch 8: loss 0.43132 acc 0.80918 auc 0.82484\u001b[0m\n",
      "loss: 1.29933 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 216.27batch/s]\n",
      "\u001b[93maverage test of epoch 8: loss 0.41731 acc 0.81714 auc 0.85112\u001b[0m\n",
      "loss: 0.10039 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 100.05batch/s]\n",
      "\u001b[92maverage training of epoch 9: loss 0.40437 acc 0.82038 auc 0.84334\u001b[0m\n",
      "loss: 1.92926 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 219.74batch/s]\n",
      "\u001b[93maverage test of epoch 9: loss 0.42350 acc 0.78286 auc 0.85555\u001b[0m\n",
      "loss: 0.00007 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 101.79batch/s]\n",
      "\u001b[92maverage training of epoch 10: loss 0.39924 acc 0.83520 auc 0.84208\u001b[0m\n",
      "loss: 1.72487 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 220.14batch/s]\n",
      "\u001b[93maverage test of epoch 10: loss 0.40873 acc 0.83143 auc 0.85263\u001b[0m\n",
      "loss: 1.55167 acc: 0.00000: 100%|███████| 2767/2767 [00:27<00:00, 100.58batch/s]\n",
      "\u001b[92maverage training of epoch 11: loss 0.38137 acc 0.84062 auc 0.85357\u001b[0m\n",
      "loss: 2.48833 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 220.27batch/s]\n",
      "\u001b[93maverage test of epoch 11: loss 0.45948 acc 0.81857 auc 0.85530\u001b[0m\n",
      "loss: 0.02544 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 101.53batch/s]\n",
      "\u001b[92maverage training of epoch 12: loss 0.37555 acc 0.84351 auc 0.85861\u001b[0m\n",
      "loss: 1.92284 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 220.94batch/s]\n",
      "\u001b[93maverage test of epoch 12: loss 0.39304 acc 0.85429 auc 0.86474\u001b[0m\n",
      "loss: 0.10724 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 100.81batch/s]\n",
      "\u001b[92maverage training of epoch 13: loss 0.37423 acc 0.84966 auc 0.85030\u001b[0m\n",
      "loss: 2.27893 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 218.88batch/s]\n",
      "\u001b[93maverage test of epoch 13: loss 0.41456 acc 0.83286 auc 0.85595\u001b[0m\n",
      "loss: 0.64617 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 101.56batch/s]\n",
      "\u001b[92maverage training of epoch 14: loss 0.37398 acc 0.84243 auc 0.85693\u001b[0m\n",
      "loss: 1.85953 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 220.42batch/s]\n",
      "\u001b[93maverage test of epoch 14: loss 0.40492 acc 0.83286 auc 0.84474\u001b[0m\n",
      "loss: 0.09438 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 102.13batch/s]\n",
      "\u001b[92maverage training of epoch 15: loss 0.37255 acc 0.84893 auc 0.85960\u001b[0m\n",
      "loss: 1.87615 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 217.85batch/s]\n",
      "\u001b[93maverage test of epoch 15: loss 0.39351 acc 0.85286 auc 0.85875\u001b[0m\n",
      "loss: 0.32850 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 100.72batch/s]\n",
      "\u001b[92maverage training of epoch 16: loss 0.38027 acc 0.84279 auc 0.85134\u001b[0m\n",
      "loss: 1.58863 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 219.01batch/s]\n",
      "\u001b[93maverage test of epoch 16: loss 0.48443 acc 0.79714 auc 0.82806\u001b[0m\n",
      "loss: 0.09505 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 101.16batch/s]\n",
      "\u001b[92maverage training of epoch 17: loss 0.38183 acc 0.84026 auc 0.84887\u001b[0m\n",
      "loss: 2.32691 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 221.39batch/s]\n",
      "\u001b[93maverage test of epoch 17: loss 0.44390 acc 0.83000 auc 0.85709\u001b[0m\n",
      "loss: 0.16122 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 101.83batch/s]\n",
      "\u001b[92maverage training of epoch 18: loss 0.36823 acc 0.84749 auc 0.85929\u001b[0m\n",
      "loss: 1.19004 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 221.78batch/s]\n",
      "\u001b[93maverage test of epoch 18: loss 0.41815 acc 0.84143 auc 0.86562\u001b[0m\n",
      "loss: 0.27142 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 101.55batch/s]\n",
      "\u001b[92maverage training of epoch 19: loss 0.37541 acc 0.84930 auc 0.85326\u001b[0m\n",
      "loss: 2.46590 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 219.78batch/s]\n",
      "\u001b[93maverage test of epoch 19: loss 0.42651 acc 0.85143 auc 0.86294\u001b[0m\n",
      "loss: 0.25276 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 102.33batch/s]\n",
      "\u001b[92maverage training of epoch 20: loss 0.36573 acc 0.85399 auc 0.85419\u001b[0m\n",
      "loss: 2.07085 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 220.80batch/s]\n",
      "\u001b[93maverage test of epoch 20: loss 0.43877 acc 0.83714 auc 0.85412\u001b[0m\n",
      "loss: 0.13523 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 102.09batch/s]\n",
      "\u001b[92maverage training of epoch 21: loss 0.36852 acc 0.84966 auc 0.85718\u001b[0m\n",
      "loss: 2.08901 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 218.51batch/s]\n",
      "\u001b[93maverage test of epoch 21: loss 0.42531 acc 0.85143 auc 0.85920\u001b[0m\n",
      "loss: 0.30108 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 100.94batch/s]\n",
      "\u001b[92maverage training of epoch 22: loss 0.37345 acc 0.84821 auc 0.85908\u001b[0m\n",
      "loss: 1.55665 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 219.43batch/s]\n",
      "\u001b[93maverage test of epoch 22: loss 0.42007 acc 0.83714 auc 0.86675\u001b[0m\n",
      "loss: 0.48298 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 100.53batch/s]\n",
      "\u001b[92maverage training of epoch 23: loss 0.38778 acc 0.83412 auc 0.84887\u001b[0m\n",
      "loss: 2.01051 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 218.98batch/s]\n",
      "\u001b[93maverage test of epoch 23: loss 0.45264 acc 0.82000 auc 0.84913\u001b[0m\n",
      "loss: 0.18348 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 101.20batch/s]\n",
      "\u001b[92maverage training of epoch 24: loss 0.36693 acc 0.85255 auc 0.85468\u001b[0m\n",
      "loss: 1.91659 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 217.95batch/s]\n",
      "\u001b[93maverage test of epoch 24: loss 0.39320 acc 0.85714 auc 0.86461\u001b[0m\n",
      "loss: 0.40137 acc: 1.00000: 100%|███████| 2767/2767 [00:27<00:00, 102.42batch/s]\n",
      "\u001b[92maverage training of epoch 25: loss 0.35704 acc 0.85761 auc 0.86489\u001b[0m\n",
      "loss: 2.37902 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 221.47batch/s]\n",
      "\u001b[93maverage test of epoch 25: loss 0.38665 acc 0.85429 auc 0.86206\u001b[0m\n",
      "loss: 0.15136 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 102.62batch/s]\n",
      "\u001b[92maverage training of epoch 26: loss 0.37202 acc 0.84496 auc 0.85388\u001b[0m\n",
      "loss: 1.92916 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 220.59batch/s]\n",
      "\u001b[93maverage test of epoch 26: loss 0.44335 acc 0.82429 auc 0.84833\u001b[0m\n",
      "loss: 0.48038 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 102.60batch/s]\n",
      "\u001b[92maverage training of epoch 27: loss 0.37915 acc 0.84062 auc 0.85406\u001b[0m\n",
      "loss: 2.79769 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 221.15batch/s]\n",
      "\u001b[93maverage test of epoch 27: loss 0.50552 acc 0.81714 auc 0.84329\u001b[0m\n",
      "loss: 0.09348 acc: 1.00000: 100%|███████| 2767/2767 [00:26<00:00, 102.63batch/s]\n",
      "\u001b[92maverage training of epoch 28: loss 0.37619 acc 0.84568 auc 0.85345\u001b[0m\n",
      "loss: 2.30079 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 221.85batch/s]\n",
      "\u001b[93maverage test of epoch 28: loss 0.40801 acc 0.85000 auc 0.86954\u001b[0m\n",
      "loss: 0.91517 acc: 0.00000: 100%|███████| 2767/2767 [00:27<00:00, 101.97batch/s]\n",
      "\u001b[92maverage training of epoch 29: loss 0.35534 acc 0.85472 auc 0.86972\u001b[0m\n",
      "loss: 2.65939 acc: 0.00000: 100%|█████████| 700/700 [00:03<00:00, 215.76batch/s]\n",
      "\u001b[93maverage test of epoch 29: loss 0.44753 acc 0.85571 auc 0.86053\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Try with AUC now\n",
    "!./run_DGCNN.sh DFDC 1 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
